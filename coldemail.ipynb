{"cells":[{"cell_type":"code","execution_count":null,"id":"f109960e-3d23-44cc-b525-c4afac091659","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f109960e-3d23-44cc-b525-c4afac091659","executionInfo":{"status":"ok","timestamp":1753661017765,"user_tz":420,"elapsed":11025,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"ff52b020-2933-4c0d-bd46-3cd77664e5af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-groq\n","  Downloading langchain_groq-0.3.6-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-groq) (0.3.71)\n","Collecting groq<1,>=0.29.0 (from langchain-groq)\n","  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.29.0->langchain-groq) (4.14.1)\n","Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-groq) (0.4.8)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-groq) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-groq) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-groq) (6.0.2)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-groq) (25.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.29.0->langchain-groq) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain-groq) (2025.7.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain-groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.29.0->langchain-groq) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-groq) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (3.11.0)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (2.32.3)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain-groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain-groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain-groq) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-groq) (2.5.0)\n","Downloading langchain_groq-0.3.6-py3-none-any.whl (16 kB)\n","Downloading groq-0.30.0-py3-none-any.whl (131 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: groq, langchain-groq\n","Successfully installed groq-0.30.0 langchain-groq-0.3.6\n"]}],"source":["pip install langchain-groq"]},{"cell_type":"markdown","id":"7fcb9821-edc5-4ed7-b284-cd53291a70e5","metadata":{"id":"7fcb9821-edc5-4ed7-b284-cd53291a70e5"},"source":["gorq is a class which langchain framework offers, langchain is framework which makes building llms apps easiler, its open-source\n"]},{"cell_type":"code","execution_count":null,"id":"27a3cb46-20ef-4e55-b96b-7dca63fccd74","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"27a3cb46-20ef-4e55-b96b-7dca63fccd74","executionInfo":{"status":"ok","timestamp":1753661020621,"user_tz":420,"elapsed":2854,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"8adcc668-4b0b-47a0-81b7-f797d5e4dfb5"},"outputs":[{"output_type":"stream","name":"stdout","text":["The first person to land on the moon was Neil Armstrong. He stepped out of the lunar module Eagle and onto the moon's surface on July 20, 1969, as part of the Apollo 11 mission. Armstrong famously declared, \"That's one small step for man, one giant leap for mankind,\" as he became the first human to set foot on the moon.\n"]}],"source":["from langchain_groq import ChatGroq\n","\n","llm = ChatGroq(\n","    model= \"llama-3.3-70b-versatile\",\n","    groq_api_key = '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',\n","    temperature=0\n","\n",")\n","response = llm.invoke(\"The first person to land on moon was\")\n","print(response.content)"]},{"cell_type":"code","source":["!pip install -q langchain langchain-community\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sZjUQ-HiWVKF","executionInfo":{"status":"ok","timestamp":1753661153410,"user_tz":420,"elapsed":10409,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"08d58579-6587-4c44-8144-8b96324f9c30"},"id":"sZjUQ-HiWVKF","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m1.9/2.5 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from langchain_community.document_loaders import WebBaseLoader\n","loader = WebBaseLoader(\"https://careers.nike.com/senior-data-and-analytics-insights-analyst/job/R-62523\")\n","page_data = loader.load().pop().page_content\n","print(page_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJqAnFycWfq_","executionInfo":{"status":"ok","timestamp":1753661188125,"user_tz":420,"elapsed":870,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"322e7df4-eb8f-4555-ef34-efd697db32e6"},"id":"lJqAnFycWfq_","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Senior Data and Analytics Insights Analyst\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Skip to main content\n","Open Virtual Assistant\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Home\n","\n","\n","Career Areas\n","\n","\n","Total Rewards\n","\n","\n","Life@Nike\n","\n","\n","Purpose\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Language\n","\n","\n","\n","\n","\n","Select a Language\n","\n","  Deutsch  \n","  English  \n","  Español (España)  \n","  Español (América Latina)  \n","  Français  \n","  Italiano  \n","  Nederlands  \n","  Polski  \n","  Tiếng Việt  \n","  Türkçe  \n","  简体中文  \n","  繁體中文  \n","  עִברִית  \n","  한국어  \n","  日本語  \n","\n","\n","\n","\n","\n","\n","\n","\n","Careers\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Close Menu\n","\n","\n","\n","\n","\n","\n","\n","Careers\n","\n","\n","\n","\n","\n","\n","Chat\n","\n","\n","\n","\n","\n","\n","                                Home\n","                            \n","\n","\n","\n","                                Career Areas\n","                            \n","\n","\n","\n","                                Total Rewards\n","                            \n","\n","\n","\n","                                Life@Nike\n","                            \n","\n","\n","\n","                                Purpose\n","                            \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Jordan Careers\n","\n","\n","\n","\n","\n","\n","\n","Converse Careers\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Language\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Menu\n","\n","\n","\n","Return to Previous Menu\n","\n","\n","\n","Select a Language\n","\n","  Deutsch  \n","  English  \n","  Español (España)  \n","  Español (América Latina)  \n","  Français  \n","  Italiano  \n","  Nederlands  \n","  Polski  \n","  Tiếng Việt  \n","  Türkçe  \n","  简体中文  \n","  繁體中文  \n","  עִברִית  \n","  한국어  \n","  日本語  \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","                        Back to Search\n","\n","                    \n","\n","Senior Data and Analytics Insights Analyst\n","\n","\n","Categories ID\n","\n","\n","\n","Categories URL\n","\n","\n","\n","Position Type\n","\n","\n","\n","Date Posted\n","\n","\n","\n","Primary Quest ID\n","\n","\n","\n","Second Quest ID\n","\n","\n","\n","Job Classification\n","\n","\n","\n","\n","Career area\n","Data and Analytics Insights\n","\n","\n","Location\n","1 Bowerman Drive, Beaverton, Oregon 97005, United States\n","\n","\n","\n","Job ID\n","R-62523\n","\n","\n","\n","\n","\n","                            Apply Now\n","                        \n","\n","\n","\n","\n","\n","\n","Share Job\n","\n","\n","\n","\n","Share Job Posting\n","\n","\n","\n","\n","\n","\n","Facebook\n","Opens In A New Tab\n","\n","\n","\n","\n","\n","\n","\n","LinkedIn\n","Link Opens In New Window\n","\n","\n","\n","\n","\n","\n","\n","\n","Email\n","\n","\n","\n","\n","\n","\n","Close-Medium (Default Size)-icon\n","\n","Close Menu\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Become a Part of the NIKE, Inc. Team\n","NIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore\n","                            potential, obliterate boundaries\n","                            and push out the edges of what can be. The company looks for people who can grow, think,\n","                            dream and create. Its\n","                            culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers,\n","                            leaders and\n","                            visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a\n","                            challenging and constantly\n","                            evolving game.\n","\n","Senior Data and Analytics Insights Analyst - NIKE, Inc. - Beaverton, OR. Design, develop, and implement new decision support solutions that help the business leverage/operationalize analytical insights; use expertise with business and technology to translate business needs into technical requirements helping the business product owner be successful and the data engineers be efficient; work within the Finance Analytics team to build data and reporting solutions supporting Nike’s transition to state of the art Sales and Financial ERP; gather requirements and design data flows to intake financial transactions, develop and test complex business rules for cost attribution, and identify and model data against additional product and customer attribution to support financial optimization and deep analytics; responsible for building the financial analytic capabilities that transform Nike to meet the challenges of a mixed retail and wholesale future; help the business Product Owner create and drive an agile product backlog within the team to deliver transformational reporting and analytics; and work with the wider product team to drive roadmaps that provide a larger solution viewpoint and align to partner expectations. Telecommuting is available from anywhere in the U.S., except from AK, AL, AR, DE, HI, IA, ID, IN, KS, KY, LA, MT, ND, NE, NH, NM, NV, OH, OK, RI, SD, VT, WV, and WY.  Employer will accept a Master’s degree in Economics, Business Administration, Information Technology, or Engineering Management and two (2) years of experience in the job offered or in an analyst-related occupation.   Experience must include:  • SQL • Python • Spark • Airflow • AWS • Snowflake • Cognos • Tableau • Relational databases.   Apply at www.Nike.com/Careers (Job# R-62523)  #LI-DNIWe offer a number of accommodations to complete our interview process including screen readers, sign language interpreters, accessible and single location for in-person interviews, closed captioning, and other reasonable modifications as needed. If you discover, as you navigate our application process, that you need assistance or an accommodation due to a disability, please complete the Candidate Accommodation Request Form.\n","\n","NIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a\n","                            generous total rewards\n","                            package, casual work environment, a diverse and inclusive culture, and an electric\n","                            atmosphere for professional\n","                            development. No matter the location, or the role, every Nike employee shares one galvanizing\n","                            mission: To bring\n","                            inspiration and innovation to every athlete* in the world.\n","NIKE, Inc. is an equal opportunity employer. Qualified applicants will receive\n","                            consideration without\n","                            regard to race, color, religion, sex, national origin, age, sexual orientation, gender\n","                            identity, gender expression,\n","                            veteran status, or disability.\n","\n","\n","\n","\n","                                Apply Now\n","                            \n","\n","\n","\n","\n","\n","\n","Share Job\n","\n","\n","\n","\n","Share Job Posting\n","\n","\n","\n","\n","\n","\n","Faceboox\n","Opens In A New Tab\n","\n","\n","\n","\n","\n","\n","\n","LinkedIn\n","Link Opens In New Window\n","\n","\n","\n","\n","\n","\n","\n","\n","Email\n","\n","\n","\n","\n","\n","\n","Close-Medium (Default Size)-icon\n","\n","Close Menu\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","What You Can Expect\n","OUR HIRING GAME PLAN\n","\n","\n","01 Apply\n","Our teams are made up of diverse skillsets, knowledge bases, inputs, ideas and backgrounds.\n","                            We want you to find your fit – review job descriptions, departments and teams to discover\n","                            the role for you.\n","\n","\n","02 Meet a Recruiter or Take an Assessment\n","If selected for a corporate role, a recruiter will reach out to start your interview process\n","                            and be your main contact\n","                            throughout the process. For retail roles, you’ll complete an interactive assessment that\n","                            includes a chat and quizzes and\n","                            takes about 10-20 minutes to complete.  No matter the role, we want to learn about you – the\n","                            whole you – so don’t shy\n","                            away from how you approach world-class service and what makes you unique.\n","\n","\n","03 Interview\n","Go into this stage confident by doing your research, understanding what we are looking for\n","                            and being prepared for\n","                            questions that are set up to learn more about you, and your background.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Home\n","\n","\n","About Us\n","\n","\n","Contact\n","\n","\n","Talent Community\n","\n","\n","Terms\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","              \n","                   \n","                    Nike Applicant Privacy Policy\n","                    \n","\n","\n","\n","\n","\n","We offer a number of accommodations to complete our interview process including screen readers, sign language interpreters,\n","                accessible and single location for in-person interviews, closed captioning, and other reasonable modifications as\n","                needed.\n","\n","\n","If you discover, as you navigate our application process, that you need assistance or an accommodation due to a\n","                disability, please contact us at +1 503-671-4156 and include your full name, best way to reach you, and the\n","                accommodation you request to assist with the application process.\n","For more information, please refer to Equal Employment\n","                        Opportunity is The Law.\n","\n","\n","\n","©  Nike, Inc. All Rights Reserved\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Careers\n","\n","\n","\n","\n","\n","\n","\n","\n","Chat\n","Chat with our AI Assistant\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]},{"cell_type":"code","execution_count":null,"id":"28b456ea-bb21-4a47-89bd-5184d2865aa7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"28b456ea-bb21-4a47-89bd-5184d2865aa7","executionInfo":{"status":"ok","timestamp":1753661723932,"user_tz":420,"elapsed":715,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"01a18c25-51d6-40ab-dd72-b1dda718ec5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["```json\n","{\n","  \"role\": \"Senior Data and Analytics Insights Analyst\",\n","  \"experience\": \"2 years\",\n","  \"location\": \"1 Bowerman Drive, Beaverton, Oregon 97005, United States\",\n","  \"skills\": [\n","    \"SQL\",\n","    \"Python\",\n","    \"Spark\",\n","    \"Airflow\",\n","    \"AWS\",\n","    \"Snowflake\",\n","    \"Cognos\",\n","    \"Tableau\",\n","    \"Relational databases\"\n","  ],\n","  \"description\": \"Design, develop, and implement new decision support solutions that help the business leverage/operationalize analytical insights; use expertise with business and technology to translate business needs into technical requirements helping the business product owner be successful and the data engineers be efficient; work within the Finance Analytics team to build data and reporting solutions supporting Nike’s transition to state of the art Sales and Financial ERP\"\n","}\n","```\n"]}],"source":["from langchain_core.prompts import PromptTemplate\n","prompt_extract = PromptTemplate.from_template(\n","    \"\"\"\n","    ### SCRAPED TEXT FROM WEBSITE:\n","    {page_data}\n","    ### INSTRUCTION:\n","    The scraped text is from the career's page of a website.\n","    your job is to extract the job postings and return them in JSON format containg the following\n","    keys: `role`, `experience`,  `location`, `skills`, and `description`.\n","    Only return the valid JSON.\n","    ### VALID JSON (No PREAMBLE):\n","    \"\"\"\n",")\n","chain_extract = prompt_extract | llm\n","res = chain_extract.invoke(input={'page_data': page_data})\n","print(res.content)"]},{"cell_type":"code","source":["from langchain_core.output_parsers import JsonOutputParser\n","\n","json_parser = JsonOutputParser()\n","json_res = json_parser.parse(res.content)\n","json_res"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MsElNJorY3Gq","executionInfo":{"status":"ok","timestamp":1753661862694,"user_tz":420,"elapsed":31,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"69536cc1-3714-4663-911d-9d715cbc9d29"},"id":"MsElNJorY3Gq","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'role': 'Senior Data and Analytics Insights Analyst',\n"," 'experience': '2 years',\n"," 'location': '1 Bowerman Drive, Beaverton, Oregon 97005, United States',\n"," 'skills': ['SQL',\n","  'Python',\n","  'Spark',\n","  'Airflow',\n","  'AWS',\n","  'Snowflake',\n","  'Cognos',\n","  'Tableau',\n","  'Relational databases'],\n"," 'description': 'Design, develop, and implement new decision support solutions that help the business leverage/operationalize analytical insights; use expertise with business and technology to translate business needs into technical requirements helping the business product owner be successful and the data engineers be efficient; work within the Finance Analytics team to build data and reporting solutions supporting Nike’s transition to state of the art Sales and Financial ERP'}"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["job = json_res\n","job['skills']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLVgpi5ipHpp","executionInfo":{"status":"ok","timestamp":1753666074960,"user_tz":420,"elapsed":20,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"d58a95a7-b27b-4e52-f89f-f8f692fee793"},"id":"fLVgpi5ipHpp","execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['SQL',\n"," 'Python',\n"," 'Spark',\n"," 'Airflow',\n"," 'AWS',\n"," 'Snowflake',\n"," 'Cognos',\n"," 'Tableau',\n"," 'Relational databases']"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["job"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhUV9oD1pOI2","executionInfo":{"status":"ok","timestamp":1753666088620,"user_tz":420,"elapsed":27,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"c87986e7-72ce-4a42-bc02-8bd27638f6a5"},"id":"nhUV9oD1pOI2","execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'role': 'Senior Data and Analytics Insights Analyst',\n"," 'experience': '2 years',\n"," 'location': '1 Bowerman Drive, Beaverton, Oregon 97005, United States',\n"," 'skills': ['SQL',\n","  'Python',\n","  'Spark',\n","  'Airflow',\n","  'AWS',\n","  'Snowflake',\n","  'Cognos',\n","  'Tableau',\n","  'Relational databases'],\n"," 'description': 'Design, develop, and implement new decision support solutions that help the business leverage/operationalize analytical insights; use expertise with business and technology to translate business needs into technical requirements helping the business product owner be successful and the data engineers be efficient; work within the Finance Analytics team to build data and reporting solutions supporting Nike’s transition to state of the art Sales and Financial ERP'}"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"-YMfVayzfmeQ","executionInfo":{"status":"ok","timestamp":1753664410616,"user_tz":420,"elapsed":4334,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"370b3b2d-eb92-4d15-a48f-c61c4e4fc06d"},"id":"-YMfVayzfmeQ","execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-87630b8c-2912-4104-ab28-fd1b69d43fe7\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-87630b8c-2912-4104-ab28-fd1b69d43fe7\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving my_portfolio.csv to my_portfolio.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Try using ISO-8859-1 (Latin-1) encoding\n","df = pd.read_csv(\"my_portfolio.csv\", encoding=\"ISO-8859-1\")\n","df.head()\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"gCyNgoBueVMc","executionInfo":{"status":"ok","timestamp":1753664585917,"user_tz":420,"elapsed":74,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"cfb19473-871c-4b42-c71c-48cb2cebe991"},"id":"gCyNgoBueVMc","execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                       Roles  \\\n","0             Data Scientist   \n","1  Healthcare Data Scientist   \n","2   Financial Data Scientist   \n","3         Decision Scientist   \n","4          Applied Scientist   \n","\n","                                              Skills  \\\n","0  Python, R, SQL, Machine Learning, Deep Learnin...   \n","1  Python, R, Bioinformatics, NLP, EHR Systems, M...   \n","2  Python, SQL, Risk Modeling, Time Series Analys...   \n","3  SQL, Python, Business Intelligence, Experiment...   \n","4  Python, PyTorch, TensorFlow, Statistics, A/B T...   \n","\n","                                          Experience  \n","0  Isha Agrawal\\n680-356-8543 | ishaagrawal2000@g...  \n","1  Isha Agrawal\\n680-356-8543 | ishaagrawal2000@g...  \n","2  Isha Agrawal\\n680-356-8543 | ishaagrawal2000@g...  \n","3  Isha Agrawal\\n680-356-8543 | ishaagrawal2000@g...  \n","4  Isha Agrawal\\n680-356-8543 | ishaagrawal2000@g...  "],"text/html":["\n","  <div id=\"df-92773f4c-9f37-4e38-b347-17b8fab9df63\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Roles</th>\n","      <th>Skills</th>\n","      <th>Experience</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Data Scientist</td>\n","      <td>Python, R, SQL, Machine Learning, Deep Learnin...</td>\n","      <td>Isha Agrawal\\n680-356-8543 | ishaagrawal2000@g...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Healthcare Data Scientist</td>\n","      <td>Python, R, Bioinformatics, NLP, EHR Systems, M...</td>\n","      <td>Isha Agrawal\\n680-356-8543 | ishaagrawal2000@g...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Financial Data Scientist</td>\n","      <td>Python, SQL, Risk Modeling, Time Series Analys...</td>\n","      <td>Isha Agrawal\\n680-356-8543 | ishaagrawal2000@g...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Decision Scientist</td>\n","      <td>SQL, Python, Business Intelligence, Experiment...</td>\n","      <td>Isha Agrawal\\n680-356-8543 | ishaagrawal2000@g...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Applied Scientist</td>\n","      <td>Python, PyTorch, TensorFlow, Statistics, A/B T...</td>\n","      <td>Isha Agrawal\\n680-356-8543 | ishaagrawal2000@g...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-92773f4c-9f37-4e38-b347-17b8fab9df63')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-92773f4c-9f37-4e38-b347-17b8fab9df63 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-92773f4c-9f37-4e38-b347-17b8fab9df63');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-7cb86703-b58e-48f5-83f4-a30ac3e986aa\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7cb86703-b58e-48f5-83f4-a30ac3e986aa')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-7cb86703-b58e-48f5-83f4-a30ac3e986aa button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 36,\n  \"fields\": [\n    {\n      \"column\": \"Roles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"Cloud Data Engineer\",\n          \"Operations Analyst\",\n          \"ML Engineer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Skills\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 36,\n        \"samples\": [\n          \"Azure, AWS, GCP, Python, Terraform\",\n          \"SQL, Excel, Reporting, Business Process Analysis\",\n          \"Scikit-learn, XGBoost, MLOps, Feature Engineering, Docker\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Experience\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Isha Agrawal\\n680-356-8543 | ishaagrawal2000@gmail.com | LinkedIn | GitHub | San Jose, CA, USA\\nEDUCATION\\nSyracuse University, School of Information Studies, Syracuse, NY Aug 2023 \\u00d0 May 2025\\nMaster of Science in Information Systems| Advance Study in Data Science\\nCERTIFICATIONS & COURSES\\nNVIDIA-Certified Associate, Academy Accreditation (Generative AI Fundamentals (Databricks)), AWS Certified Cloud Practitioner, Deep\\nDive into LLMs, Hands-On Essentials: Data Warehousing Workshop, Generative AI for Everyone (DeepLearning.AI)\\nWORK EXPERIENCE\\nData Scientist, Healthcare Data Operations & Reporting | Persistent Systems| Mumbai, India Aug 2022 \\u00d0 Jun 2023\\n_ Resolved delays in clinical decisions caused by fragmented EHR, claims, and lab data by engineering HIPAA-compliant healthcare\\npipelines using Python, SQL, and Pandas. Cut data retrieval time by 40% and enabled real-time patient summaries\\n_ Built predictive healthcare analytics models using Random Forest, XGBoost, and SDOH inputs to forecast disease risk and\\nreadmissions. Improved prediction accuracy to 85%, supporting decision tools and proactive, value-based care\\n_ Integrated AI-driven anomaly detection for claims adjudication, using Isolation Forest models to flag outliers in billing and coding\\ndata. Prevented $500K+ in revenue leakage and optimized reporting for finance and compliance\\n_ Developed population health dashboards in Tableau, automating risk score, readmission, and care quality KPI reporting. Helped\\nclinicians reduce patient safety events by 20% and cut manual reporting by 500+ hours\\n_ Partnered with clinical, IT, and compliance teams to build scalable data workflows aligned with interoperability and HIPAA\\nmandates. Improved audit-readiness reduced regulatory risks, and accelerated KPI reporting\\n_ Designed scalable ML pipelines with PySpark and SQL for 1M+ patient records, using cross-validation and backtesting to boost\\nmodel performance. Accelerated care decisions and reduced readmissions by 15%\\nCloud Engineer, Healthcare Data Operations & Reporting | Persistent Systems| Mumbai, India July 2021 \\u00d0 Aug 2022\\n_ Designed scalable, serverless APIs using AWS API Gateway, Lambda, and gRPC, integrating with AWS RDS to securely unify\\nhealthcare data access for cross-functional clinical teams, reducing patient data retrieval bottlenecks across departments.\\n_ Automated CI/CD pipelines with Jenkins, GitLab CI/CD, and AWS CodePipeline, reducing manual errors and improving deployment\\nreliability for healthcare platforms, while enabling faster, seamless production rollouts for critical healthcare applications.\\n_ Boosted API performance by 40%, cut data retrieval times by 20%, and shortened deployment cycles by 25%, delivering real-time\\nclinical and patient insights via AWS QuickSight dashboards.\\n_ Collaborated with data governance and compliance teams to ensure data workflows and APIs met HIPAA and interoperability\\nmandates, reducing audit risks.\\nData Analyst | CompanyHub | Mumbai, India July 2019 \\u00d0 July 2021\\n_ Addressed data fragmentation issues impacting customer feature adoption and engagement insights for a CRM platform with 10K+\\nusers, enabling data-driven decision-making\\n_ Streamlined SQL-based ETL pipelines and developed Power BI dashboards with Python, improving data accuracy by 35%,\\nreducing reporting turnaround by 60%, and boosting feature adoption by 40%\\n_ Collaborated with product managers and engineers to align analytics with business goals, driving platform enhancements and\\ntargeted customer strategies\\nPROJECTS\\nAI Doctor assistant (Python, LLM, NLP) | Github\\n_ Developed an AI-powered medical assistant using Python, Groq multimodal LLM, Whisper STT, Llama-3.2 Vision Model,\\nElevenLabs TTS, gTTS, and Gradio UI for real-time voice and image diagnostics\\n_ Integrated APIs securely to enable seamless access to advanced speech recognition and vision models, enhancing diagnostic\\naccuracy and responsiveness\\n_ Achieved 85%+ diagnostic accuracy and reduced consultation time by 35%, improving patient engagement through lifelike\\nconversational AI\\nJob Market Analysis (Python, NLP, Tableau) | Github\\n_ Analyzed 10K+ job postings using Python, LLM-based NLP techniques (TF-IDF, NER, Word Cloud), tokenization, and topic\\nmodeling to extract salary benchmarks, skill trends, and market gaps across 15+ industries\\n_ Added AI-driven sentiment and skill demand models with Hugging Face, boosting job market trend accuracy by 20%.\\n_ Built an interactive Tableau dashboard visualizing salary ranges, job clusters, and in-demand skills for 1,000+ users to make\\ninformed career moves\\nTECHNICAL SKILLS\\nProgramming & Scripting: Python, SQL, R, C++, JavaScript, Java, Bash, SAS, Linux\\nDatabases & Data Platforms: MySQL, Oracle, Snowflake, Cassandra, Amazon RDS\\nMachine Learning & AI: Supervised/Unsupervised Learning, Predictive Modeling, Deep Learning (CNN, RNN), NLP, A/B Testing,\\nstatistical analysis, Tensorflow, Keras, Bayesian inference, NLP, LLMs, Hugging Face, Gen AI, RLFH, Agentic AI, Pre-tuning\\nCloud & DevOps: AWS, Azure, Docker, Kubernetes, CI/CD, Git, GitLab, GitHub\\nData Visualization & ETL: Tableau, Power BI, Looker, Microsoft Excel, ELT (Extract Load Transform) pipelines\",\n          \"Isha Agrawal\\n680-356-8543 | ishaagrawal2000@gmail.com | LinkedIn | GitHub | San Jose, CA, USA\\nEDUCATION\\nSyracuse University, School of Information Studies, Syracuse, NY Aug 2023 \\u00d0 May 2025\\nMaster of Science in Information Systems| Advance Study in Data Science\\nCERTIFICATIONS & COURSES\\nHands-On Essentials: Data Warehousing Workshop (Snowflake)| Microsoft Azure Data Engineer (DP-203) | AWS Certified Cloud\\nPractitioner | NVIDIA-Certified Associate | Academy Accreditation (Generative AI Fundamentals (Databricks))\\nWORK EXPERIENCE\\nSoftware Engineer, Financial Operations & Reporting | Persistent Systems Jan 2022 \\u00d0 Jun 2023\\n_ Deployed and managed the backend infrastructure of a high-traffic banking website on AWS EKS, ensuring high availability,\\nscalability, and reliability of cloud-native services powering critical financial APIs for loan eligibility and decisioning\\n_ Designed ELT pipelines using Apache Airflow and Snowflake to ingest and transform API logs, telemetry, powering real-time\\ndashboards, compliance reporting, and operational insights\\n_ Automated infrastructure provisioning with Terraform, managing VPCs, IAM roles, subnets, load balancers, and Kubernetes\\nclusters to ensure consistent, secure, and efficient deployments\\n_ Implemented observability and monitoring tools using Prometheus, OpenTelemetry, CloudWatch, and Grafana to track API latency\\nand system health, improving reliability and reducing downtime\\n_ Enhanced performance through CI/CD automation and testing, using GitLab CI, AWS CodePipeline, ArgoCD, JMeter, and Gatling,\\ncutting chatbot response time by 30% and compute costs by 20%\\nSoftware Engineer, Financial Operations & Reporting | Persistent Systems Jul 2021 \\u00d0 Jan 2022\\n_ Led a QA team of 5 engineers supporting a large-scale, high-traffic financial loan decisioning platform; managed over 1,500 manual\\ntest cases using Zephyr in Jira and resolved critical data validation issues causing frequent release delays and inconsistent report\\n_ Designed and implemented modular UI and API automation frameworks (Selenium, Java, TestNG, Cucumber BDD, Cypress, Rest\\nAssured), improving automation coverage from 45% to 80% and reducing manual regression testing effort by 60% per release\\n_ Integrated automated test suites into Jenkins CI/CD pipelines, enabling parallel test execution that cut test feedback loop time by\\n50% and accelerated deployment cycles by 30%\\n_ Developed advanced test strategies covering edge cases and fault injections, reducing critical post-release bugs by 25% and\\nimproving pre-release issue detection rate by 45%\\n_ Facilitated cross-functional collaboration with developers, product managers, business analysts, and client teams to align release\\ngoals, test coverage, and sprint risks, leading to a 25% drop in production bugs and improved SLA adherence\\n_ Defined and monitored key QA metrics such as defect leakage rate, test case effectiveness, coverage gaps, and automation ROI to\\nguide sprint planning and refine test design, while ensuring adherence to the full Software Development Life Cycle (SDLC)\\n_ Mentored 10+ QA engineers, boosting team automation contribution rate by 40% and reducing review turnaround time by 35%\\nthrough code reviews and collaborative development sessions\\n_ Improved test execution reliability and scalability by applying robust test data management practices and leveraging SQL Server for\\nbackend data validations under high-load and edge-case testing conditions\\nPROJECTS\\nAI Doctor Assistant (Python, Multimodal LLM, Gradio)\\n_ Developed an AI medical assistant using Python, Groq\\u00d5s Multimodal LLM, Whisper STT, and LLaMA-3.2 Vision for real-time\\ndiagnostics & patient interaction. Built a Gradio UI for seamless speech, image, and text queries, delivering AI-driven insights\\n_ Integrated ElevenLabs TTS and gTTS APIs for lifelike voice feedback, enhancing user experience and accessibility, with future\\nenhancements planned for advanced image recognition and expanded medical knowledge integration\\nNetflix clone (React JS)\\n_ Developed a Netflix Clone using React.js, Firebase, and TMDB API, implementing user authentication, dynamic movie\\nretrieval, and responsive UI for an interactive streaming experience\\n_ Integrated REST APIs using Axios to fetch and display real-time movie data, utilizing React Router for seamless\\nnavigation and Bootstrap for enhanced UI/UX\\nLLM Evaluation Framework for Chatbot Testing\\n_ Built a lightweight evaluation pipeline using Hugging Face\\u00d5s Phi-3 model to test chatbot responses across key metrics like fluency,\\ncoherence, completeness, relevance, and toxicity\\n_ Integrated semantic similarity, sentiment analysis, and toxicity detection to generate automated response scores and simulate\\nCI/CD testing via Jenkins using pre-generated evaluation CSVs\\nTECHNICAL SKILLS\\nProgramming & Scripting: Python, R, SQL, SAS, Bash, Java, C++\\nCloud Platforms: AWS (S3, Lambda, RDS), Azure, GCP\\nData & Databases: MySQL, Snowflake, Oracle, Amazon RDS, Dimensional Modeling, ETL, ELT\\nAnalytics & Visualization: Power BI, Tableau, AWS QuickSight, Excel (Pivot Tables, VLOOKUP), EDA, A/B Testing\\nDevOps & Infrastructure: Docker, Kubernetes, Terraform, Git, GitHub, Azure DevOps\\nML & AI Applications: Supervised/Unsupervised Learning, Predictive Modeling, NLP, LLMs, Generative AI, Deep Learning\",\n          \"Isha Agrawal\\n680-356-8543 | ishaagrawal2000@gmail.com | LinkedIn | GitHub | San Jose, CA, USA\\nEDUCATION\\nSyracuse University, School of Information Studies, Syracuse, NY Aug 2023 \\u00d0 May 2025\\nMaster of Science in Information Systems| Advance Study in Data Science\\nCERTIFICATIONS & COURSES\\nNVIDIA-Certified Associate, Academy Accreditation (Generative AI Fundamentals (Databricks)), AWS Certified Cloud Practitioner,\\nDeep Dive into LLMs, Hands-On Essentials: Data Warehousing Workshop, Generative AI for Everyone (DeepLearning.AI)\\nWORK EXPERIENCE\\nData Scientist, Financial Operations & Reporting | Persistent Systems Aug 2022 \\u00d0 Jun 2023\\n_ Resolved processing delays in a real-time loan approval system handling 1M+ applications annually by automating risk\\nscoring models using Random Forest and XGBoost, reducing financial exposure from manual assessments and improving\\noperational decision-making speed by 70%\\n_ Designed and deployed scalable ML pipelines using Spark for high-volume loan data; implemented cross-validation, data\\nmining, and time-based backtesting strategies to optimize model accuracy and ensure robust risk predictions\\n_ Developed an AI-driven anomaly detection framework using Isolation Forest to flag irregular loan applications in real-time,\\nreducing fraudulent approvals and strengthening fraud prevention and compliance monitoring processes by 30%\\n_ Collaborated with data engineering teams to optimize Python, SQL, and Spark-based ETL workflows for structured, high-\\nvolume financial data pipelines, improving data preparation efficiency by 40% while ensuring operational reporting\\naccuracy, data quality, and audit readiness\\n_ Led model monitoring and performance visualization using Tableau dashboards, enabling real-time tracking of approval\\nrates, default risks, and financial KPIs for C-suite leaders and operational managers\\nCloud Engineer, Financial Operations & Reporting | Persistent Systems July 2021 \\u00d0 Aug 2022\\n_ Deployed and managed the backend infrastructure of a high-traffic banking website on AWS EKS, ensuring high availability,\\nscalability, and reliability of cloud-native services powering critical financial APIs for loan eligibility and decisioning\\n_ Designed ELT pipelines using Apache Airflow and Snowflake to ingest and transform API logs, telemetry, powering real-time\\ndashboards, compliance reporting, and operational insights\\n_ Automated infrastructure provisioning with Terraform, managing VPCs, IAM roles, subnets, load balancers, and Kubernetes\\nclusters to ensure consistent, secure, and efficient deployments\\n_ Implemented observability and monitoring tools using Prometheus, CloudWatch, and Grafana to track API latency and\\nsystem health, improving reliability and reducing downtime\\n_ Enhanced performance through CI/CD automation and testing, using GitLab CI, AWS CodePipeline, JMeter, and Gatling,\\ncutting chatbot response time by 30% and compute costs by 20%\\nData Analyst | CompanyHub July 2019 \\u00d0 July 2021\\n_ Addressed data fragmentation issues impacting customer feature adoption and engagement insights for a CRM platform\\nwith 10K+ users, enabling data-driven decision-making\\n_ Streamlined SQL-based ETL pipelines and developed Power BI dashboards with Python, improving data accuracy by 35%,\\nreducing reporting turnaround by 60%, and boosting feature adoption by 40%\\n_ Collaborated with product managers and engineers to align analytics with business goals, driving platform enhancements\\nand targeted customer strategies\\nPROJECTS\\nAI Doctor assistant (Python, LLM, NLP) | Github\\n_ Developed an AI-powered medical assistant using Python, Groq multimodal LLM, Whisper STT, Llama-3.2 Vision Model,\\nElevenLabs TTS, gTTS, and Gradio UI for real-time voice and image diagnostics\\n_ Integrated APIs securely to enable seamless access to advanced speech recognition and vision models. This enhanced\\ndiagnostic accuracy, achieving 85%+ accuracy and reducing consultation time by 35%\\nJob Market Analysis (Python, NLP, Tableau) | Github\\n_ Analyzed 10K+ job postings using Python, LLM-based NLP techniques (TF-IDF, NER, Word Cloud), tokenization, and topic\\nmodeling to extract salary benchmarks, skill trends, and market gaps across 15+ industries\\n_ Built an interactive Tableau dashboard visualizing salary ranges, job clusters, and in-demand skills for 1,000+ users to make\\ninformed career moves\\nTECHNICAL SKILLS\\nProgramming & Scripting: Python, SQL, R, C++, JavaScript, Java, Bash, SAS, Linux\\nDatabases & Data Platforms: MySQL, Oracle, Snowflake, Cassandra, Amazon RDS\\nMachine Learning & AI: Supervised/Unsupervised Learning, Predictive Modeling, Deep Learning (CNN, RNN), NLP, A/B Testing,\\nstatistical analysis, Tensorflow, Keras, Bayesian inference, NLP, LLMs, Hugging Face, Gen AI, RLFH, Agentic AI, Pre-tuning\\nCloud & DevOps: AWS, Azure, Docker, Kubernetes, CI/CD, Git, GitLab, GitHub\\nData Visualization & ETL: Tableau, Power BI, Looker, Microsoft Excel, ELT (Extract Load Transform) pipelines\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["pip install chromadb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZOJTKxqlbRr","executionInfo":{"status":"ok","timestamp":1753665121442,"user_tz":420,"elapsed":25189,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"168b7d91-2107-4453-e70f-cddafb81cfa8"},"id":"PZOJTKxqlbRr","execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting chromadb\n","  Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n","Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n","Collecting pybase64>=1.4.1 (from chromadb)\n","  Downloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n","Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n","Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n","Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n","  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.1)\n","Collecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n","Collecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n","Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n","Collecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.0)\n","Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n","Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\n","Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (25.0)\n","Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.7.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n","Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n","Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n","  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n","Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n","Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.35.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n","Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.4)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n","Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n","Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n","Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.2)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n","Downloading chromadb-1.0.15-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.35.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_exporter_otlp_proto_common-1.35.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.35.0-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n","Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n","Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pypika\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=2cef827faf25a01d0eeb515817878a62c36f37ac552a179476e531b62d00a984\n","  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n","Successfully built pypika\n","Installing collected packages: pypika, durationpy, uvloop, pybase64, overrides, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, opentelemetry-semantic-conventions, onnxruntime, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n","Successfully installed backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.15 coloredlogs-15.0.1 durationpy-0.10 httptools-0.6.4 humanfriendly-10.0 kubernetes-33.1.0 mmh3-5.1.0 onnxruntime-1.22.1 opentelemetry-api-1.35.0 opentelemetry-exporter-otlp-proto-common-1.35.0 opentelemetry-exporter-otlp-proto-grpc-1.35.0 opentelemetry-proto-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 uvloop-0.21.0 watchfiles-1.1.0\n"]}]},{"cell_type":"code","source":["import chromadb\n","import uuid\n","\n","client=chromadb.PersistentClient('vectorstore')\n","collection = client.get_or_create_collection(name = \"portfolio\")\n","\n","\n","if not collection.count():\n","  for _, row in df.iterrows():\n","    collection.add(\n","      documents=[f\"Role: {row['Roles']}\\nSkills: {row['Skills']}\\nExperience: {row['Experience']}\"],\n","      metadatas={\"role\": row[\"Roles\"], \"skills\": row[\"Skills\"], \"experience\": row[\"Experience\"]},\n","      ids=[str(uuid.uuid4())]\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-vn9tOzjk14","executionInfo":{"status":"ok","timestamp":1753665244104,"user_tz":420,"elapsed":22233,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"956ccf96-d322-4e53-890e-edaca2005783"},"id":"Z-vn9tOzjk14","execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:02<00:00, 34.2MiB/s]\n"]}]},{"cell_type":"code","source":["# Lowercase for normalization\n","query_text = job['role'].lower()\n","\n","# Query ChromaDB using just the role\n","experience_results = collection.query(query_texts=[query_text], n_results=2)\n","\n","# Extract only matching experience\n","experience = \"\\n\\n\".join([\n","    meta['experience'] for meta in experience_results['metadatas'][0]\n","    if query_text in meta['role'].lower()\n","])\n","\n"],"metadata":{"id":"4L20g1U4mQcU","executionInfo":{"status":"ok","timestamp":1753666976842,"user_tz":420,"elapsed":571,"user":{"displayName":"Isha","userId":"15351510584285401752"}}},"id":"4L20g1U4mQcU","execution_count":45,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0dG2kAjYnC6n"},"id":"0dG2kAjYnC6n","execution_count":null,"outputs":[]},{"cell_type":"code","source":["job = json_res\n","job['skills']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qgAN8SRnC9T","executionInfo":{"status":"ok","timestamp":1753665553151,"user_tz":420,"elapsed":20,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"09c0cebe-3c91-47fd-dbd5-9150c5bc022c"},"id":"9qgAN8SRnC9T","execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['SQL',\n"," 'Python',\n"," 'Spark',\n"," 'Airflow',\n"," 'AWS',\n"," 'Snowflake',\n"," 'Cognos',\n"," 'Tableau',\n"," 'Relational databases']"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["from langchain.prompts import PromptTemplate\n","\n","prompt_email = PromptTemplate.from_template(\n","\"\"\"\n","### JOB DESCRIPTION:\n","{job_description}\n","\n","### YOUR EXPERIENCE:\n","{experience}\n","\n","### INSTRUCTION:\n","You are an enthusiastic job seeker writing a cold email for the role above.\n","\n","Only use the experience and job description provided. Do **not** invent any job titles, numbers, or achievements not mentioned. Stick to facts.\n","\n","Write an email that:\n","- Begins with: “Hey, I hope you are doing well.”\n","- Then says: “Hi, I just came across the {{role}} position and I believe I’d be a great fit. Here’s why:”\n","- Includes **3 bullet points**, each 2–3 lines\n","    - Start each bullet with a phrase like:\n","        - “Your role is focused on…”\n","        - “The job mentions…”\n","        - “You’re looking for someone skilled in…”\n","    - Link it clearly to the provided experience. Do not fabricate anything.\n","- Ends with: “Would you be open to a quick coffee chat this week to explore this further?”\n","- Followed by: “Thank you and warm regards,\n","Isha Agrawal”\n","- Then include:\n","    Email: ishaagrawal2000@gmail.com\n","    GitHub: https://github.com/IshaAg07\n","\n","Keep the tone confident, real, and respectful. Do not provide a preamble.\n","\n","### EMAIL (NO PREAMBLE):\n","\"\"\"\n",")\n","\n","\n","\n","\n","chain_email = prompt_email | llm\n","\n","res = chain_email.invoke({\n","    \"job_description\": str(job),\n","    \"experience\": experience\n","})\n","\n","print(res.content)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7043n2pncat","executionInfo":{"status":"ok","timestamp":1753667399310,"user_tz":420,"elapsed":1235,"user":{"displayName":"Isha","userId":"15351510584285401752"}},"outputId":"1a39aeb1-d4df-425a-8747-62cd46ec11fd"},"id":"s7043n2pncat","execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Hey, I hope you are doing well.\n","Hi, I just came across the Senior Data and Analytics Insights Analyst position and I believe I’d be a great fit. Here’s why:\n","* Your role is focused on designing, developing, and implementing new decision support solutions, which aligns with my interest in using analytical insights to drive business decisions. I'm excited about the opportunity to work on such projects. My skills in SQL, Python, and relational databases could be valuable in this context.\n","* The job mentions working within the Finance Analytics team to build data and reporting solutions, which suggests a collaborative environment that I'd thrive in. I'm looking forward to the chance to contribute to Nike’s transition to a state-of-the-art Sales and Financial ERP. This opportunity to work with a team on significant projects is appealing.\n","* You’re looking for someone skilled in technologies like Spark, Airflow, AWS, Snowflake, Cognos, and Tableau, which are all relevant to my experience with data analysis and insights. I'm confident that my skills in these areas would enable me to make a positive impact in this role. I'm excited about the prospect of applying my knowledge to drive business success.\n","Would you be open to a quick coffee chat this week to explore this further?\n","Thank you and warm regards,  \n","Isha Agrawal\n","Email: ishaagrawal2000@gmail.com  \n","GitHub: https://github.com/IshaAg07\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0B_YQmBFuL0t"},"id":"0B_YQmBFuL0t","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python [conda env:base] *","language":"python","name":"conda-base-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}